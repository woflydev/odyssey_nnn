{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"http://colab.research.google.com/github/woflydev/odyssey_nnn/blob/main/utils/notebooks/training.ipynb\">\n",
    "        <img src=\"https://i.ibb.co/2P3SLwK/colab.png\"  style=\"padding-bottom:5px;\" />Run in Google Colab</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "# from skimage import data\n",
    "from skimage import io\n",
    "from skimage.exposure import match_histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input image resolution to the DNN model (smaller than the original dataset image resolution: 320x240x3)\n",
    "# new camera has width and height of (640, 360)\n",
    "img_width = 640\n",
    "img_height = 360\n",
    "img_channels = 3\n",
    "\n",
    "model_name = \"model_opt\"\n",
    "model_file = \"models/{}-{}x{}x{}\".format(model_name[6:], img_width, img_height, img_channels)\n",
    "pre=\"resize\" # [resize|crop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "dataset_dir = \"dataset\"\n",
    "dataset_zipfile = 'Dataset-kucsl-Apr2022.zip' # 'labtrack-leftonly-RTCSA22.zip' # 'Dataset.zip'\n",
    "RunningInCOLAB = 'google.colab' in str(get_ipython())\n",
    "force_unzip = True\n",
    "use_match_histogram = False\n",
    "\n",
    "if RunningInCOLAB: \n",
    "    print(\"in colab\")\n",
    "    from google.colab import files\n",
    "    uploaded = files.upload()\n",
    "    for fn in uploaded.keys():\n",
    "        print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
    "            name=fn, length=len(uploaded[fn])))\n",
    "        dataset_zipfile = fn  # expecting a zip file to be uploaded. \n",
    "else:\n",
    "    print (\"not in colab\")\n",
    "    # RPI_IP=None # replace this with the IP address of your raspberry pi \n",
    "    # if RPI_IP is not None:\n",
    "    #     URL=\"http://\"+RPI_IP+\":8000/\"+dataset_zipfile\n",
    "    #     filename=URL.split('/')[-1]\n",
    "    #     r = requests.get(URL)\n",
    "    #     open(filename, 'wb').write(r.content)\n",
    "\n",
    "if not os.path.isfile(dataset_zipfile):\n",
    "    #default_dataset_URL = \"https://raw.githubusercontent.com/heechul/DeepPicar-v3/devel/Dataset-kucsl-Apr2022.zip\"    \n",
    "    ##print (\"No existing dataset. download from \", default_dataset_URL)\n",
    "    #r = requests.get(default_dataset_URL)\n",
    "    #open(dataset_zipfile, 'wb').write(r.content)\n",
    "    pass\n",
    "\n",
    "if not os.path.isdir(dataset_dir) or force_unzip==True:\n",
    "    #print ('unzip %s into %s folder' % (dataset_zipfile, dataset_dir))\n",
    "    #zip_ref = zipfile.ZipFile(dataset_zipfile)\n",
    "    #zip_ref.extractall(dataset_dir)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this first\n",
    "#!pip uninstall --yes gdown # After running this line, restart Colab runtime.\n",
    "\n",
    "# then this\n",
    "!pip install --upgrade --no-cache-dir gdown\n",
    "\n",
    "# replace this with id from gdrive\n",
    "!gdown --id 1Q4-JTe_lxXonSl2VjDHz7FEjJjZjHvV3\n",
    "\n",
    "# unzip\n",
    "!unzip dataset.zip -d dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_match_histogram:\n",
    "    refs = []\n",
    "    num_spectrums = 5\n",
    "    size = img_width*img_height\n",
    "    num_colors = int(256 / num_spectrums)\n",
    "    color_pixels = int(size / num_colors)\n",
    "    color = 0\n",
    "    for s in range(num_spectrums): \n",
    "        num_color = 0\n",
    "        img = []\n",
    "        for i in range(img_height):\n",
    "            img.append([])\n",
    "            for j in range(img_width):\n",
    "                img[i].append([])\n",
    "                #for k in range(3):\n",
    "                img[i][j].append(color)\n",
    "                img[i][j].append(color)\n",
    "                img[i][j].append(color)\n",
    "                num_color += 1\n",
    "                if num_color == color_pixels - 1:\n",
    "                    if not color == 255:\n",
    "                        color += 1\n",
    "                    num_color = 0            \n",
    "        img = np.asarray(img, dtype=np.uint8) / 255.\n",
    "        refs.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test data lists\n",
    "imgs = []\n",
    "vals = []\n",
    "\n",
    "def rad2deg(rad):\n",
    "\treturn 180.0 * rad / math.pi\n",
    "\n",
    "def crop_image(img):\n",
    "\tscaled_img = cv2.resize(img, (max(int(img_height * 4 / 3), img_width), img_height))\n",
    "\tfb_h, fb_w, fb_c = scaled_img.shape\n",
    "\t# print(scaled_img.shape)\n",
    "\tstartx = int((fb_w - img_width) / 2)\n",
    "\tstarty = int((fb_h - img_height) / 2)\n",
    "\treturn scaled_img[starty:starty+img_height, startx:startx+img_width,:]\n",
    "\n",
    "def data_generator():\n",
    "\tfor vid_file_path in glob.iglob(f'{dataset_dir}/*.mp4'):\n",
    "\t\tcsv_file_path = vid_file_path.replace('video', 'key').replace('mp4', 'csv') #used to be avi but different encoding now\n",
    "\t\tprint(vid_file_path, csv_file_path)\n",
    "\t\t\n",
    "\t\tvid = cv2.VideoCapture(vid_file_path)\n",
    "\t\tdf = read_csv(csv_file_path)\n",
    "\t\t\n",
    "\t\tfor val in df[\"wheel\"].values:\n",
    "\t\t\tret,img = vid.read()\n",
    "\t\t\tif ret is not False:\n",
    "\t\t\t\tif pre == \"crop\":\n",
    "\t\t\t\t\timg = crop_image(img)\n",
    "\t\t\t\t\tprint(img.shape)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\timg = cv2.resize(img, (img_width, img_height))\n",
    "\n",
    "\t\t\t\t# Convert to grayscale and readd channel dimension\n",
    "\t\t\t\tif img_channels == 1:\n",
    "\t\t\t\t\t\timg = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\t\t\t\t\t\timg = np.reshape(img, (img_height, img_width, img_channels))\n",
    "\t\t\t\timg = (img / 255).astype(np.uint8) # <--- prevents ram overload\n",
    "\n",
    "\t\t\t\tif use_match_histogram == True:\n",
    "\t\t\t\t\tfor i in range(num_spectrums):\n",
    "\t\t\t\t\t\timg = match_histograms(img, refs[i], multichannel=True) # channel_axis=-1\n",
    "\n",
    "\t\t\t\tyield img, tf.constant(val) # returns the frame and data\n",
    "\t\t\telse:\n",
    "\t\t\t\tprint(\"video seems to be empty. nothing to do!\")\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\t\n",
    "\n",
    "import glob\n",
    "for vid_file_path in glob.iglob(f'{dataset_dir}/*.mp4'):\n",
    "\t\tcsv_file_path = vid_file_path.replace('video', 'key').replace('mp4', 'csv') #used to be avi but different encoding now\n",
    "\t\tprint(vid_file_path, csv_file_path)\n",
    "\t\tvid = cv2.VideoCapture(vid_file_path)\n",
    "\t\tdf = read_csv(csv_file_path)\n",
    "\t\tfor val in df[\"wheel\"].values:\n",
    "\t\t\t\tret,img = vid.read()\n",
    "\t\t\t\tif ret is not False:\n",
    "\t\t\t\t\tif pre==\"crop\":\n",
    "\t\t\t\t\t\timg = crop_image(img)\n",
    "\t\t\t\t\t\tprint(img.shape)\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\timg = cv2.resize(img, (img_width, img_height))\n",
    "\t\t\t\t\t# Convert to grayscale and readd channel dimension\n",
    "\t\t\t\t\tif img_channels == 1:\n",
    "\t\t\t\t\t\t\timg = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\t\t\t\t\t\t\timg = np.reshape(img, (img_height, img_width, img_channels))\n",
    "\t\t\t\t\timg = (img / 255).astype(np.uint8) # <--- prevents ram overload\n",
    "\t\t\t\t\timgs.append(img)\n",
    "\t\t\t\t\tvals.append(val)\n",
    "\t\t\t\t\tif use_match_histogram == True:\n",
    "\t\t\t\t\t\t\tfor i in range(num_spectrums):\n",
    "\t\t\t\t\t\t\t\t\tmatched = match_histograms(img, refs[i], multichannel=True) # channel_axis=-1\n",
    "\t\t\t\t\t\t\t\t\timgs.append(matched)\n",
    "\t\t\t\t\t\t\t\t\tvals.append(val)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\t print(\"video seems to be empty. nothing to do!\")\n",
    "\t\tprint(len(imgs), len(vals))\n",
    "\n",
    "list_ds = tf.data.Dataset.from_generator(data_generator, output_signature=(\n",
    "\t\ttf.TensorSpec(shape=(img_height, img_width, img_channels), dtype=tf.float32),\n",
    "\t\ttf.TensorSpec(shape=(1), dtype = tf.float32)\n",
    "))\n",
    "\n",
    "# Convert lists to numpy arrays and ensure they are of equal length    \n",
    "imgs = np.asarray(imgs)  # input images\n",
    "vals = np.asarray(vals)  # steering angles\n",
    "assert len(imgs) == len(vals)\n",
    "print(\"Loaded {} smaples\".format(len(imgs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_match_histogram:\n",
    "    fig, axises = plt.subplots(nrows=1, ncols=num_spectrums+1, figsize=(8, 4),\n",
    "                                        sharex=True, sharey=True)\n",
    "    for i in range(num_spectrums+1):\n",
    "        aa = axises[i]\n",
    "        aa.imshow(imgs[240 + i])\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset for train and test \n",
    "print(\"Splitting the dataset for training and testing...\")\n",
    "x_train, x_test, y_train, y_test = train_test_split(imgs, vals, test_size=0.35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "\n",
    "use_depthmodel = False\n",
    "\n",
    "if use_depthmodel == True: \n",
    "  h_len = img_height\n",
    "  w_len = img_width\n",
    "  d_len = img_channels\n",
    "\n",
    "  depth_multiplier = 1.0\n",
    "  dw_conv_str=\"1111\"\n",
    "  fc_str=\"111\"\n",
    "  # depthwise convolution model (doesn't work for some reason!!!)\n",
    "  model = tf.keras.Sequential()\n",
    "  \n",
    "  dw_conv_arr = []\n",
    "  dw_conv_arr.append(layers.DepthwiseConv2D((5,5), strides=(2, 2)))\n",
    "  dw_conv_arr.append(layers.Conv2D(round(36 * depth_multiplier), (1, 1), strides=(1, 1)))\n",
    "\n",
    "  dw_conv_arr.append(layers.DepthwiseConv2D((5,5), strides=(2, 2)))\n",
    "  dw_conv_arr.append(layers.Conv2D(round(48 * depth_multiplier), (1, 1), strides=(1, 1)))\n",
    "\n",
    "  dw_conv_arr.append(layers.DepthwiseConv2D((3,3), strides=(1, 1)))\n",
    "  dw_conv_arr.append(layers.Conv2D(round(64 * depth_multiplier), (1, 1), strides=(1, 1)))\n",
    "\n",
    "  dw_conv_arr.append(layers.DepthwiseConv2D((3,3), strides=(1, 1)))\n",
    "  dw_conv_arr.append(layers.Conv2D(round(64 * depth_multiplier), (1, 1), strides=(1, 1)))\n",
    "\n",
    "  fc_arr = []\n",
    "  fc_arr.append(layers.Dense(round(100 * depth_multiplier), activation='relu'))\n",
    "  fc_arr.append(layers.Dense(round(50 * depth_multiplier), activation='relu'))\n",
    "  fc_arr.append(layers.Dense(round(10 * depth_multiplier), activation='relu'))\n",
    "\n",
    "  # input layer\n",
    "  model.add(layers.Input(shape=(h_len,w_len,d_len), name='input'))\n",
    "  model.add(layers.DepthwiseConv2D((5,5), strides=(2, 2), name='dwi'))\n",
    "  model.add(layers.Conv2D(round(24 * depth_multiplier), (1, 1), strides=(1, 1), name='pwi'))\n",
    "  model.add(layers.BatchNormalization())\n",
    "  model.add(layers.ReLU())\n",
    "\n",
    "  # Add Conv2D layers\n",
    "  # for i in range(len(conv_str)):\n",
    "  # \tif int(conv_str[i]) == 1:\n",
    "  # \t\tmodel.add(conv2d_arr[i])\n",
    "\n",
    "  for i in range(0, len(dw_conv_str)):\n",
    "    if int(dw_conv_str[i]) == 1:\n",
    "      model.add(dw_conv_arr[i*2])\n",
    "      model.add(dw_conv_arr[i*2+1])\n",
    "      model.add(layers.BatchNormalization())\n",
    "      model.add(layers.ReLU())\n",
    "\n",
    "  # Add Flatten layer\n",
    "  model.add(layers.Flatten())\n",
    "\n",
    "  # Add FC layers\n",
    "  for i in range(len(fc_str)):\n",
    "    if int(fc_str[i]) == 1:\n",
    "      model.add(fc_arr[i])\n",
    "\n",
    "  # Add final output layer\n",
    "  model.add(layers.Dense(1, name=\"output\"))\n",
    "\n",
    "else:\n",
    "  # original model\n",
    "  model = tf.keras.Sequential()\n",
    "  model.add(layers.Conv2D(24, (5,5), strides=(2,2), activation='relu', input_shape=(img_height,img_width,img_channels)))\n",
    "  model.add(layers.Conv2D(36, (5,5), strides=(2,2), activation='relu'))\n",
    "  model.add(layers.Conv2D(48, (5,5), strides=(2,2), activation='relu'))\n",
    "  model.add(layers.Conv2D(64, (3,3), activation='relu'))\n",
    "  model.add(layers.Conv2D(64, (3,3), activation='relu'))\n",
    "  model.add(layers.Flatten())\n",
    "  model.add(layers.Dense(100, activation='relu'))\n",
    "  model.add(layers.Dense(50, activation='relu'))\n",
    "  model.add(layers.Dense(10, activation='relu'))\n",
    "  model.add(layers.Dense(1, name=\"output\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Quantize to make it extreme zoom...\")\n",
    "if RunningInCOLAB:\n",
    "  !pip install tensorflow_model_optimization\n",
    "\n",
    "import tensorflow_model_optimization as tfmot\n",
    "quantize_model = tfmot.quantization.keras.quantize_model\n",
    "q_aware_model = quantize_model(model)\n",
    "\n",
    "print (\"Compiling model...\")\n",
    "q_aware_model.compile(optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "\t\t                loss=tf.keras.losses.MeanSquaredError())\n",
    "q_aware_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Model fit\")\n",
    "# q_aware_model.reset_states()\n",
    "history = q_aware_model.fit(x_train, y_train, batch_size=128,\n",
    "                    epochs=50, validation_data=(x_test, y_test))\n",
    "\n",
    "# Plot training and validation losses \n",
    "print(history.history.keys())\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angle_thresh=10\n",
    "# Helper functions for evaluating model accuracy\n",
    "\n",
    "def get_action(angle_rad):\n",
    "  degree = rad2deg(angle_rad)\n",
    "  if degree < angle_thresh and degree > -angle_thresh:\n",
    "    return \"center\"\n",
    "  elif degree >= angle_thresh:\n",
    "    return \"right\" \n",
    "  elif degree <-angle_thresh:\n",
    "    return \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_angle = q_aware_model.predict(x_test)\n",
    "pred = np.array(list(map(get_action, pred_angle)))\n",
    "ground = np.array(list(map(get_action, y_test)))\n",
    "print('Accuracy is %.3f' % np.mean(pred == ground))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(i, prediction_array, groundtrue_array, img_array):\n",
    "  angle, predicted_label, groundtrue_label, img = int(rad2deg(pred_angle[i])), prediction_array[i], groundtrue_array[i], img_array[i]\n",
    "  plt.grid(True)\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "  img = np.squeeze(img)\n",
    "  plt.imshow(img, cmap=plt.cm.binary)\n",
    "  if predicted_label == groundtrue_label:\n",
    "    color = 'green'\n",
    "  else:\n",
    "    color = 'red'  \n",
    "  plt.xlabel(\"Predict:{} ({}) Actual:{}\".format(predicted_label,angle,groundtrue_label), color=color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Visualize the outputs { run: \"auto\" }\n",
    "index = 20 #@param {type:\"slider\", min:0, max:244, step:6}\n",
    "for i in range(3):\n",
    "  plt.figure(figsize=(10,3))\n",
    "  plt.subplot(1,2,1)\n",
    "  plot_image(index + 2*i, pred, ground, x_test)\n",
    "  plt.subplot(1,2,2)\n",
    "  plot_image(index + 2*i+1, pred, ground, x_test)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save both the Keras and TFLite models      \n",
    "print(\"Model Save\")                  \n",
    "q_aware_model.save(model_file+\".h5\")\n",
    "files.download(model_file+\".h5\")\n",
    "\n",
    "print(\"TFLite Model\")\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)\n",
    "\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "# def representative_data_gen():\n",
    "#   for input_value in tf.data.Dataset.from_tensor_slices(imgs).batch(1).take(100):\n",
    "#     yield [input_value]\n",
    "\n",
    "# converter.representative_dataset = representative_data_gen\n",
    "# converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "# converter.inference_input_type = tf.int8  # or tf.uint8\n",
    "# converter.inference_output_type = tf.int8  # or tf.uint8\n",
    "\n",
    "quantized_tflite_model = converter.convert()\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_content=quantized_tflite_model)\n",
    "input_type = interpreter.get_input_details()[0]['dtype']\n",
    "print('input: ', input_type)\n",
    "output_type = interpreter.get_output_details()[0]['dtype']\n",
    "print('output: ', output_type)\n",
    "\n",
    "with open(model_file+\".tflite\", 'wb') as f:\n",
    "    f.write(quantized_tflite_model)\n",
    "\n",
    "if RunningInCOLAB: \n",
    "    from google.colab import files\n",
    "    files.download(model_file+\".tflite\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
